#!python
import logging
import multiprocessing as mp
import os
import random
from datetime import date, datetime
from functools import partial
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
from alive_progress import alive_bar

PROJECT_ROOT = Path.cwd()

# Use dynamic years based on available data

current_year = datetime.now().year

# Configuration with environment variable support for testing
# Production mode by default - test mode must be explicitly enabled
QUICK_TEST = os.getenv('QUICK_TEST', 'false').lower() == 'true'
USE_MULTIPROCESSING = os.getenv('USE_MULTIPROCESSING', 'true').lower() == 'true'
MAX_CORES = int(os.getenv('MAX_CORES', str(mp.cpu_count())))

# Hybrid model predictions fallback store
HYBRID_PREDICTIONS = None

if QUICK_TEST:
    print("üöÄ QUICK TEST MODE ENABLED")
    my_years = [current_year - 2, current_year - 1]  # Only last 2 years
    number_of_simulations = 200  # Much faster for testing
    print(f"   Using {number_of_simulations} simulations with {len(my_years)} years")
else:
    # Use centralized training configuration for consistency
    from ffbayes.utils.training_config import get_monte_carlo_training_years
    my_years = get_monte_carlo_training_years()  # Last 5 years (2020-2024)
    number_of_simulations = 5000
    print(f"   Using {number_of_simulations} simulations with {len(my_years)} years")
    print(f"   Years: {my_years}")

if USE_MULTIPROCESSING:
    print(f"üî• MULTIPROCESSING ENABLED: Using {MAX_CORES} cores")
else:
    print("üîÑ Single-threaded execution")

todays_date = date.today()
logging.basicConfig()

# Initialize combined data to None
combined_data = None


### Use Monte Carlo simulation to project the score of my team, based on historical data ###
### From Scott Rome: https://srome.github.io/Making-Fantasy-Football-Projections-Via-A-Monte-Carlo-Simulation/ ###

def get_combined_data(directory_path):
    """Get combined data from the unified dataset."""
    try:
        from ffbayes.data_pipeline.unified_data_loader import load_unified_dataset
        
        print("   üîÑ Loading unified dataset...")
        df = load_unified_dataset(directory_path)
        print(f"   ‚úÖ Loaded {len(df):,} rows from unified dataset")
        print(f"   ‚úÖ Available years: {sorted(df['Season'].unique())}")
        return df
        
    except Exception as e:
        print(f"   ‚ùå Error loading unified dataset: {e}")
        raise ValueError(f"Failed to load unified dataset: {e}")


def load_hybrid_predictions() -> dict:
    """Load Hybrid MC Bayesian predictions for fallback sampling."""
    try:
        from ffbayes.utils.path_constants import get_hybrid_mc_dir
        hyb_dir = get_hybrid_mc_dir(current_year)
        hyb_path = hyb_dir / 'hybrid_model_results.json'
        if not hyb_path.exists():
            print(f"   ‚ö†Ô∏è  Hybrid predictions not found at {hyb_path}")
            return {}
        import json
        with open(hyb_path, 'r') as f:
            data = json.load(f)
        print(f"   ‚úÖ Loaded Hybrid MC predictions for {len(data)} players")
        return data
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Failed to load Hybrid MC predictions: {e}")
        return {}


def sample_from_hybrid(player_name: str) -> Optional[float]:
    """Sample a score for a player from Hybrid MC predictive distribution."""
    global HYBRID_PREDICTIONS
    if not HYBRID_PREDICTIONS or player_name not in HYBRID_PREDICTIONS:
        return None
    pdata = HYBRID_PREDICTIONS[player_name]
    mc = pdata.get('monte_carlo', {})
    mean = mc.get('mean')
    std = mc.get('adjusted_std') or mc.get('std')
    if mean is None or std is None:
        return None
    # Draw from Normal(mean, std), truncated at 0
    draw = float(np.random.normal(mean, max(std, 1e-6)))
    return max(0.0, draw)


# Try to load team file from my_ff_teams directory

def find_latest_team_file() -> str:
    """Find the most recent team file generated by the draft strategy."""
    # CRITICAL: Configurable results path - no hardcoding
    # Priority: 1. Environment variable, 2. Default based on QUICK_TEST
    results_base = os.getenv('RESULTS_PATH')
    if not results_base:
        if QUICK_TEST:
            from ffbayes.utils.path_constants import get_default_team_file
            results_base = str(get_default_team_file())
            print(f"   QUICK_TEST mode - using test results path: {results_base}")
        else:
            # Production mode - use the my_ff_teams directory
            from ffbayes.utils.path_constants import get_teams_dir
            results_base = str(get_teams_dir())
            print(f"   Production mode - using my_ff_teams directory: {results_base}")
    
    draft_strategy_dir = Path(results_base)
    if not draft_strategy_dir.exists():
        return None
    
    # Look for team files with pattern 'drafted_team_*.tsv'
    team_files = list(draft_strategy_dir.glob('drafted_team_*.tsv'))
    
    if not team_files:
        return None
    
    # Sort by modification time and return the most recent
    team_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return str(team_files[0])


def load_team_from_file(team_file: str) -> pd.DataFrame:
    """Load team from a provided TSV file and sanitize columns."""
    if not os.path.exists(team_file):
        raise FileNotFoundError(f"Team file not found: {team_file}")
    team_df = pd.read_csv(team_file, sep='\t')
    
    # Handle different column name formats
    if 'POS' in team_df.columns:
        team_df = team_df.rename(columns={'POS': 'Position'})
    elif 'Position' not in team_df.columns:
        raise ValueError(
            f"Team file missing Position column. Available columns: {team_df.columns.tolist()}\n"
            f"Expected: POS or Position column. Fix your team file format."
        )
    
    # Handle player name column
    if 'PLAYER' in team_df.columns:
        team_df = team_df.rename(columns={'PLAYER': 'Name'})
    elif 'Name' not in team_df.columns:
        raise ValueError(
            f"Team file missing player name column. Available columns: {team_df.columns.tolist()}\n"
            f"Expected: PLAYER or Name column. Fix your team file format."
        )
    
    # Team info will be derived from the unified dataset during processing
    # No fallbacks - if we can't find it, the pipeline should break
    
    # Filter valid positions - now includes team defense and flex positions
    valid_positions = ['QB', 'RB', 'WR', 'TE', 'DEF', 'FLEX', 'BE']
    team_df = team_df[team_df['Position'].isin(valid_positions)]
    
    # Clean up player names (remove position suffixes from bench players)
    team_df['Name'] = team_df['Name'].str.replace(r'\s*\([^)]+\)', '', regex=True)
    
    # Use the original team file directly - no need for duplicate files
    try:
        from ffbayes.utils.name_resolver import create_name_resolver
        resolver = create_name_resolver()
        print("üîç Resolving player names for database matching...")
        resolved_df, resolution_log = resolver.resolve_team_names(team_df)
        resolver.print_resolution_summary(resolution_log)
        # Name validation log already saved by validate_draft_team.py
        return resolved_df
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Name resolution failed: {e}")
        print("   Using original names (may cause matching issues)")
        return team_df


def make_team(team, db):
    """
    Make my team based off of my picks and whether they have historical data to simulate on
    """
    tm = []
    # Convert pandas Series to set more explicitly
    my_team_names = set(team['Name'].tolist())
    valid_positions = set(['QB', 'WR', 'TE', 'RB', 'DEF', 'FLEX', 'BE'])
    
    # Handle both 'Position' and 'position' column names for compatibility
    position_col = 'Position' if 'Position' in db.columns else 'position'
    
    # Try exact matching first
    filtered_db = db[db['Name'].isin(my_team_names) & db[position_col].isin(valid_positions)]
    
    # Check which players were found and which were missing
    found_players = set(filtered_db['Name'].unique())
    missing_players = my_team_names - found_players
    
    if missing_players:
        print(f"‚ö†Ô∏è  Warning: {len(missing_players)} players not found in historical database:")
        for player in missing_players:
            print(f"   - {player} (likely a rookie or new player)")
        print(f"   Continuing with {len(found_players)} players who have historical data")
    
    # If no players found at all, that's a critical error
    if filtered_db.empty:
        raise ValueError(
            f"‚ùå CRITICAL ERROR: No players found in database for team: {my_team_names}\n"
            f"üí° This means the pipeline cannot run Monte Carlo validation.\n"
            f"üîç Check that player names in your team file match the database format.\n"
            f"üìä Expected format examples: 'Patrick Mahomes' not 'P. Mahomes'"
        )
    
    # Remove duplicates and select only the required columns
    # Handle different column names for compatibility
    position_col = 'Position' if 'Position' in filtered_db.columns else 'position'
    team_col = 'Tm' if 'Tm' in filtered_db.columns else 'recent_team'
    
    # Ensure we have the required columns
    required_cols = ['Name', position_col, team_col]
    available_cols = [col for col in required_cols if col in filtered_db.columns]
    
    if len(available_cols) < 2:
        print(f"Warning: Missing required columns. Available: {filtered_db.columns.tolist()}")
        return pd.DataFrame(columns=['Name', 'Position', 'Tm'])
    
    result = filtered_db[available_cols].drop_duplicates(subset=['Name', position_col])
    
    # Rename columns to expected format
    result = result.rename(columns={
        position_col: 'Position',
        team_col: 'Tm'
    })
    
    # Team info should be available from the database lookup
    # If it's missing, that's a critical error - no fallbacks
    if 'Tm' in result.columns and result['Tm'].isna().any():
        missing_teams = result[result['Tm'].isna()]['Name'].tolist()
        raise ValueError(
            f"‚ùå CRITICAL ERROR: Missing team information for players: {missing_teams}\n"
            f"üí° This indicates a data integrity issue in the unified dataset.\n"
            f"üîç The pipeline cannot proceed without complete player data."
        )
    
    return result


def validate_team(db_team, my_team):
    """
    Validate the team by checking if all players in my team have historical data to simulate on
    """
    ### Check which members of my team actually have historical data to simulate on ###
    # get the column names of team using team.dtype.names
    # Handle both 'Position' and 'position' column names for compatibility
    position_col = 'Position' if 'Position' in db_team.columns else 'position'
    unique_teams = db_team.loc[:, ['Name', position_col, 'Tm']].drop_duplicates()

    # Convert pandas Series to sets more explicitly
    db_set = set(unique_teams['Name'].tolist())
    my_team_set = set(my_team['Name'].tolist())
    print('Players to project: ')
    # Convert Series to list before printing to avoid mock comparison issues
    print(unique_teams['Name'].tolist())

    # Fix pandas comparison issue by converting to sets first
    if db_set == my_team_set:
        print('Found all team members.')
    else:
        print('\nMissing team members:')
        missing_players = my_team_set.difference(db_set)
        print(missing_players)


def get_games(db, year, week):
    ### return all the players in this week and this year
    if week is None:
        # Return all games for the year
        result = db[db['Season'] == year]
    else:
        # Return games for specific week
        result = db[(db['Season'] == year) & (db['G#'] == week)]
    return result


def score_player(p, db, year, week):
    """
    Score a player for a specific year and week.
    Enhanced with better error handling.
    """
    try:
        sc = db.loc[(db['Name'] == p.Name) & (db['Season'] == year) & (db['G#'] == week)]
        if sc.empty:
            raise IndexError(f"No data found for {p.Name} in {year} week {week}")
        
        fantasy_points = sc['FantPt'].tolist()
        if not fantasy_points:
            raise IndexError(f"No fantasy points found for {p.Name} in {year} week {week}")
        
        final_sc = fantasy_points[0]
        if pd.isna(final_sc):
            raise ValueError(f"Fantasy points is NaN for {p.Name} in {year} week {week}")
        
        return float(final_sc)
    except (IndexError, KeyError, ValueError) as e:
        # Re-raise with more context
        raise IndexError(f"Error scoring {p.Name}: {e}") from e


def get_score_for_player(db, player, years, max_attempts=100):
    """
    Get a fantasy football score for a player based on historical data.
    Enhanced to handle players with limited historical data.
    """
    # First, check if player exists in the database at all
    player_data = db[db['Name'] == player.Name]
    if player_data.empty:
        print(f'‚ùå CRITICAL ERROR: Player {player.Name} not found in database')
        raise RuntimeError(
            f"Player {player.Name} not found in historical database. "
            "This player may be a rookie or have no historical data."
        )
    
    # Check which years the player has data for
    available_years = player_data['Season'].unique()
    if len(available_years) == 0:
        print(f'‚ùå CRITICAL ERROR: No season data found for {player.Name}')
        raise RuntimeError(
            f"No season data found for {player.Name}. "
            "This player may be a rookie or have no historical data."
        )
    
    # Use only years where the player has data
    valid_years = [year for year in years if year in available_years]
    if not valid_years:
        print(f'‚ö†Ô∏è  Warning: {player.Name} has no data in simulation years {years}')
        print(f'   Available years: {sorted(available_years)}')
        # Use the most recent available year as fallback
        valid_years = [max(available_years)]
    
    for attempt in range(max_attempts):
        try:
            # Randomly select a year from valid years
            year = random.choice(valid_years)
            games = get_games(db, year, None)  # Get all games for the year
            
            if games.empty:
                continue
            
            # Get all games for this player in this year
            player_games = games[games['Name'] == player.Name]
            if player_games.empty:
                continue
            
            # Randomly select a week from available weeks
            available_weeks = player_games['G#'].unique()
            if len(available_weeks) == 0:
                continue
            
            week = random.choice(available_weeks)
            
            # Get the specific game for this player, year, and week
            game = player_games[player_games['G#'] == week]
            
            if not game.empty and 'FantPt' in game.columns:
                score = game['FantPt'].iloc[0]
                if pd.notna(score) and score >= 0:  # Allow 0 scores
                    return float(score)
            
        except Exception as e:
            if attempt == max_attempts - 1:
                print(f'‚ùå CRITICAL ERROR: Failed to get score for {player.Name} after {max_attempts} attempts: {e}')
                raise RuntimeError(
                    f"Failed to get historical data for {player.Name}. "
                    f"Error: {e}"
                )
            continue
    
    # If we get here, we couldn't find any valid data
    print(f'‚ùå CRITICAL ERROR: No valid historical data found for {player.Name}')
    print(f'   Available years: {sorted(available_years)}')
    print(f'   Player data shape: {player_data.shape}')
    raise RuntimeError(
        f"No valid historical data found for {player.Name}. "
        "This player may be a rookie or have insufficient historical data."
    )


def simulate_batch(team, db, years, batch_size, batch_id=0):
    """
    Run a batch of simulations for multiprocessing.
    Returns a DataFrame with simulation results.
    """
    results = []
    
    for sim in range(batch_size):
        # CRITICAL FIX: Seed each simulation with a unique random seed
        # This ensures each simulation produces different results
        unique_seed = int(hash(f"{batch_id}_{sim}_{datetime.now().timestamp()}") % 2**32)
        np.random.seed(unique_seed)
        
        team_score = 0
        player_scores = {}
        
        for player in team.itertuples():
            try:
                score = get_score_for_player(db, player, years)
            except Exception:
                # Fallback to Hybrid MC predictive distribution
                fallback_score = sample_from_hybrid(player.Name)
                if fallback_score is None:
                    # If still unavailable, propagate error
                    raise
                print(f"   üîÅ Fallback (Hybrid MC) used for {player.Name}: {fallback_score:.2f}")
                score = fallback_score
            team_score += score
            player_scores[player.Name] = score
        
        # Add total team score
        player_scores['Total'] = team_score
        results.append(player_scores)
    
    return pd.DataFrame(results)


def simulate_parallel(team, db, years, exps=10):
    """
    Enhanced simulation function with multiprocessing support.
    """
    print("üéØ Starting Parallel Monte Carlo Simulation")
    print(f"   Team size: {len(team)} players")
    print(f"   Simulations: {exps:,}")
    print(f"   Years: {years}")
    print(f"   CPU cores: {MAX_CORES}")
    print(f"   Started at: {datetime.now().strftime('%H:%M:%S')}")
    
    if USE_MULTIPROCESSING and exps >= 100:  # Only use multiprocessing for larger jobs
        # Split simulations across cores
        batch_size = exps // MAX_CORES
        remainder = exps % MAX_CORES
        
        # Create batches
        batches = [batch_size] * MAX_CORES
        for i in range(remainder):
            batches[i] += 1
        
        print(f"   Batch sizes: {batches}")
        
        # Create partial function with fixed arguments
        simulate_func = partial(simulate_batch, team, db, years)
        
        # Run in parallel
        with mp.Pool(MAX_CORES) as pool:
            batch_args = [(batch_size, i) for i, batch_size in enumerate(batches) if batch_size > 0]
            
            with alive_bar(len(batch_args), title="Parallel Simulation Batches", bar="smooth") as bar:
                results = []
                for batch_size, batch_id in batch_args:
                    result = pool.apply_async(simulate_func, (batch_size, batch_id))
                    results.append(result)
                
                # Collect results
                batch_results = []
                for result in results:
                    batch_df = result.get()
                    batch_results.append(batch_df)
                    bar()
        
        # Combine all batch results
        outcome = pd.concat(batch_results, ignore_index=True)
        
    else:
        # Fall back to single-threaded simulation
        print("   Using single-threaded execution")
        outcome = simulate(team, db, years, exps)
    
    return outcome


def simulate(team, db, years, exps=10):
    """
    Original simulation function (single-threaded).
    """
    print("üéØ Starting Monte Carlo Simulation")
    print(f"   Team size: {len(team)} players")
    print(f"   Simulations: {exps:,}")
    print(f"   Total operations: {exps * len(team):,}")
    print(f"   Years: {years}")
    print(f"   Started at: {datetime.now().strftime('%H:%M:%S')}")
    
    # Initialize tracking variables
    total_operations = exps * len(team)
    successful_scores = 0
    fallback_scores = 0
    errors = 0
    simulation_scores = []
    
    results = []
    start_time = datetime.now()
    
    with alive_bar(total_operations, bar='smooth', title='Monte Carlo Simulation',
                   dual_line=True) as bar:
        
        for exp in range(exps):
            # CRITICAL FIX: Seed each simulation with a unique random seed
            # This ensures each simulation produces different results
            unique_seed = int(hash(f"single_{exp}_{datetime.now().timestamp()}") % 2**32)
            np.random.seed(unique_seed)
            
            team_score = 0
            player_scores = {}
            
            for player in team.itertuples():
                try:
                    score = get_score_for_player(db, player, years)
                    # Track success/fallback
                    if score > 0:
                        successful_scores += 1
                    else:
                        fallback_scores += 1
                except Exception as e:
                    # Fallback to Hybrid MC predictive distribution
                    fallback_score = sample_from_hybrid(player.Name)
                    if fallback_score is None:
                        logging.warning(f"Error scoring {player.Name} and no Hybrid fallback: {e}")
                        player_scores[player.Name] = 0
                        errors += 1
                        bar()
                        continue
                    print(f"   üîÅ Fallback (Hybrid MC) used for {player.Name}: {fallback_score:.2f}")
                    score = fallback_score
                    fallback_scores += 1
                
                team_score += score
                player_scores[player.Name] = score
                
                bar()
            
            # Add total team score
            player_scores['Total'] = team_score
            results.append(player_scores)
            simulation_scores.append(team_score)
            
            # Progress reporting every 10%
            if (exp + 1) % max(1, exps // 10) == 0:
                progress_pct = ((exp + 1) / exps) * 100
                elapsed = (datetime.now() - start_time).total_seconds()
                recent_avg = np.mean(simulation_scores[-min(100, len(simulation_scores)):])
                
                print(f"\n         üìä Progress: {progress_pct:.0f}% complete ({exp + 1}/{exps} sims)")
                print(f"    Elapsed time: {elapsed:.1f}s")
                print(f"    Recent avg team score: {recent_avg:.1f}")
    
    outcome = pd.DataFrame(results)
    
    # Final statistics
    total_time = (datetime.now() - start_time).total_seconds()
    avg_score = np.mean(simulation_scores) if simulation_scores else 0
    
    print("\n‚úÖ Simulation Complete!")
    print(f"   Total time: {total_time:.1f}s")
    print(f"   Average time per simulation: {total_time/exps:.2f}s")
    print(f"   Successful scores: {successful_scores:,}")
    print(f"   Fallback scores: {fallback_scores:,}")
    print(f"   Average team score: {avg_score:.1f}")
    
    return outcome


def main(args=None):
    """
    Main function with standardized interface.
    """
    from ffbayes.utils.script_interface import create_standardized_interface
    
    interface = create_standardized_interface(
        "ffbayes-mc",
        "Fantasy Football Monte Carlo Simulation with standardized interface"
    )
    
    # Build parser and parse arguments
    parser = interface.setup_argument_parser()
    parser = interface.add_model_arguments(parser)
    parser = interface.add_data_arguments(parser)
    # Require explicit team file unless QUICK_TEST
    parser.add_argument("--team-file", type=str, help="Path to TSV team file with Name, Position, Tm columns")
    # Auto-team-file option removed - always require explicit team file
    args = parser.parse_args()
    
    # Set up logging
    logger = interface.setup_logging(args)
    
    # Parse years if provided
    years = my_years
    if args.years:
        years = interface.parse_years(args.years)
        logger.info(f"Processing specified years: {years}")
    else:
        logger.info(f"Processing default years: {years}")
    
    # Get simulations count
    simulations = number_of_simulations
    if args.quick_test:
        logger.info("Running in QUICK_TEST mode - using reduced simulations")
        simulations = min(simulations, 1000)
    
    # Override with command line arguments if provided
    if args.cores:
        global MAX_CORES
        MAX_CORES = args.cores
        logger.info(f"Using {MAX_CORES} CPU cores")
    
    logger.info(f"Simulation parameters: Years={years}, Simulations={simulations:,}")
    
    # Load team automatically - use test file for testing, production file for real drafts
    logger.info("Loading team data...")
    if args.team_file:
        my_team = interface.handle_errors(load_team_from_file, team_file=args.team_file)
    else:
        # Auto-detect team file based on mode
        if QUICK_TEST:
            # Testing mode - use dedicated test team file with same 16 players every time
            from ffbayes.utils.path_constants import get_default_team_file
            test_team_file = str(get_default_team_file())
            if os.path.exists(test_team_file):
                logger.info(f"QUICK_TEST mode - using standard test team: {test_team_file}")
                my_team = interface.handle_errors(load_team_from_file, team_file=test_team_file)
            else:
                interface.log_error(f"Test team file not found: {test_team_file}. This file should contain the same 16 players for consistent testing.", interface.EXIT_DATA_ERROR)
                return
        else:
            # Production mode - look for user's actual draft picks
            current_year = datetime.now().year
            from ffbayes.utils.path_constants import get_teams_dir
            production_team_file = str(get_teams_dir() / f'drafted_team_{current_year}.tsv')
            if os.path.exists(production_team_file):
                logger.info(f"Production mode - using user draft picks: {production_team_file}")
                my_team = interface.handle_errors(load_team_from_file, team_file=production_team_file)
            else:
                interface.log_error(f"User draft file not found: {production_team_file}. This should contain your actual draft picks. Provide --team-file or ensure draft file exists.", interface.EXIT_DATA_ERROR)
                return
    logger.info(f"Loaded {len(my_team)} players")
    
    # Create team locally for this execution
    logger.info("Processing team against historical database...")
    # Load combined data here to avoid module-level I/O during import
    global combined_data
    data_dir = interface.get_data_directory(args)
    combined_data = interface.handle_errors(get_combined_data, directory_path=str(data_dir))
    
    # Load Hybrid MC predictions for fallback
    global HYBRID_PREDICTIONS
    HYBRID_PREDICTIONS = load_hybrid_predictions()
    
    team = interface.handle_errors(make_team, team=my_team, db=combined_data)
    logger.info(f"Found {len(team)} players with historical data")
    
    if len(team) == 0:
        interface.log_error("No players found in historical database. Cannot run simulation.", interface.EXIT_DATA_ERROR)
        return
    
    # Display team roster
    logger.info("Final team roster:")
    for idx, player in team.iterrows():
        logger.info(f"  {player['Position']}: {player['Name']} ({player['Tm']})")
    
    # Run simulation with parallel processing
    start_time = datetime.now()
    outcome = interface.handle_errors(simulate_parallel, team, combined_data, years, simulations)
    total_time = (datetime.now() - start_time).total_seconds()
    
    # Validate Monte Carlo results
    logger.info("Validating Monte Carlo simulation results...")
    validation_results = interface.validate_monte_carlo_model(outcome)
    if not validation_results.get('valid', True):
        logger.warning("Monte Carlo validation issues detected")
    else:
        logger.info("Monte Carlo results validated successfully")
    
    # Generate comprehensive results summary
    logger.info("Simulation Results:")
    
    # Calculate statistics for the Total column (team scores)
    if 'Total' in outcome.columns:
        team_scores = outcome['Total']
        mean_score = team_scores.mean()
        std_score = team_scores.std()
        se_score = std_score / np.sqrt(len(team_scores))
        min_score = team_scores.min()
        max_score = team_scores.max()
        
        # 95% confidence interval
        ci_lower = mean_score - 1.96 * se_score
        ci_upper = mean_score + 1.96 * se_score
        
        logger.info(f"  Team projection: {mean_score:.2f} points")
        logger.info(f"  Standard deviation: {std_score:.2f} points")
        logger.info(f"  Standard error: {se_score:.2f}")
        logger.info(f"  Min score: {min_score:.2f} points")
        logger.info(f"  Max score: {max_score:.2f} points")
        logger.info(f"  95% confidence interval: [{ci_lower:.2f}, {ci_upper:.2f}]")
    
    logger.info("Player Performance Summary:")
    for col in outcome.columns:
        if col != 'Total':
            player_scores = outcome[col]
            logger.info(f"  {col}: {player_scores.mean():.1f} ¬± {player_scores.std():.1f} points")
    
    # Save results with draft year instead of timestamp
    logger.info("Saving results...")
    # CRITICAL: Configurable output path - no hardcoding
    # Priority: 1. Environment variable, 2. Default based on QUICK_TEST
    output_base = os.getenv('OUTPUT_PATH')
    if not output_base:
        # Always use year-based directory structure for consistency
        from ffbayes.utils.path_constants import get_monte_carlo_dir
        current_year = datetime.now().year
        output_base = str(get_monte_carlo_dir(current_year))
        if QUICK_TEST:
            print(f"   QUICK_TEST mode - using year-based path: {output_base}")
        else:
            print(f"   Production mode - using year-based path: {output_base}")
    
    output_dir = Path(output_base)
    output_dir.mkdir(parents=True, exist_ok=True)
    current_year = datetime.now().year
    
    # Use consistent naming convention
    from ffbayes.utils.file_naming import get_monte_carlo_filename

    # Use the ACTUAL years that were used in the simulation, not hardcoded config
    filename = get_monte_carlo_filename(current_year, my_years)
    output_file = output_dir / filename
    
    outcome.to_csv(output_file, sep='\t', index=False)
    logger.info(f"Results saved to: {output_file}")
    
    logger.info(f"Total execution time: {total_time:.1f} seconds")
    interface.log_completion("Monte Carlo simulation completed successfully")
    
    return outcome


if __name__ == '__main__':
    main()
